{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Imports and Installs**"
      ],
      "metadata": {
        "id": "Xv3QdSiKct0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential, save_model\n",
        "from keras.layers import Dense, Bidirectional, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "from keras.regularizers import L1L2\n",
        "from keras.optimizers import Adam\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "PoEddALocqZX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load data**"
      ],
      "metadata": {
        "id": "_6VLgeNkc2sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "       # Clean column names\n",
        "    data.columns = data.columns.str.strip()\n",
        "    data.columns = data.columns.str.replace(' ', '_')\n",
        "    data[\"date\"] = pd.to_datetime(data[\"Date\"])\n",
        "    data.set_index(\"date\", inplace=True)\n",
        "    data = data.drop(\"Date\", axis=1)\n",
        "\n",
        "    # Validate required columns\n",
        "    required_cols = ['Open', 'High', 'Low', 'Close']\n",
        "    missing = [col for col in required_cols if col not in data.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "    # Robust numeric conversion\n",
        "    for col in required_cols:\n",
        "        # Remove all non-digit characters except dots and negatives\n",
        "        data[col] = (\n",
        "            data[col].astype(str)\n",
        "            .str.replace(r'[^\\d.-]', '', regex=True)  # Improved regex\n",
        "            .replace(r'^\\.$', np.nan, regex=True)  # Handle lone dots\n",
        "            .replace('', np.nan)\n",
        "        )\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "    # Post-cleaning validation\n",
        "    print(\"\\nData types after cleaning:\")\n",
        "    print(data.dtypes)\n",
        "\n",
        "    print(\"\\nNull values after cleaning:\")\n",
        "    print(data.isna().sum())\n",
        "\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "data = load_data(\"/content/sample_data/Data/merged (2).csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyLyZejTc2Rn",
        "outputId": "90f256c8-3ca0-44f8-eb70-da5e801732c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data types after cleaning:\n",
            "Open      float64\n",
            "High      float64\n",
            "Low       float64\n",
            "Close     float64\n",
            "Sector     object\n",
            "dtype: object\n",
            "\n",
            "Null values after cleaning:\n",
            "Open      0\n",
            "High      0\n",
            "Low       0\n",
            "Close     0\n",
            "Sector    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Display sample**"
      ],
      "metadata": {
        "id": "HRHNW_M9dPA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nData Sample:\")\n",
        "print(data.head())\n",
        "print(\"Columns in DataFrame:\", data.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WXAoad5dM7T",
        "outputId": "37a6bf9f-c258-4290-bd25-fbb294b38a82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Sample:\n",
            "            Open  High   Low  Close          Sector\n",
            "date                                               \n",
            "1979-01-02  14.7  14.7  14.7   14.7  Gold price INR\n",
            "1979-01-03  14.7  14.7  14.7   14.7  Gold price INR\n",
            "1979-01-04  14.8  14.8  14.8   14.8  Gold price INR\n",
            "1979-01-05  15.1  15.1  15.1   15.1  Gold price INR\n",
            "1979-01-08  15.1  15.1  15.1   15.1  Gold price INR\n",
            "Columns in DataFrame: ['Open', 'High', 'Low', 'Close', 'Sector']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Technical Indicators**"
      ],
      "metadata": {
        "id": "jbmk_h0gdYk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add_technical_indicators(df):\n",
        "    df = df.select_dtypes(include=np.number)\n",
        "\n",
        "    # Moving Averages\n",
        "    df['SMA_10'] = df['Close'].rolling(10).mean()\n",
        "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    # RSI Calculation\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    avg_gain = gain.rolling(14).mean()\n",
        "    avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)  # Avoid division by zero\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # ATR Calculation\n",
        "    high_low = df['High'] - df['Low']\n",
        "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "    df['ATR'] = true_range.rolling(14).mean()\n",
        "\n",
        "    return df.dropna()\n",
        "\n",
        "data = add_technical_indicators(data)\n",
        "# Prepare data\n",
        "target = data[['Close']]\n",
        "features = data.drop(['Close', 'Date'], axis=1, errors='ignore')\n",
        "# Final type check\n",
        "print(\"\\nFinal feature types:\")\n",
        "print(features.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dVDUulXdXO4",
        "outputId": "37ed4bbd-33ce-4da1-858b-626b6c63baf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final feature types:\n",
            "Open      float64\n",
            "High      float64\n",
            "Low       float64\n",
            "SMA_10    float64\n",
            "EMA_10    float64\n",
            "RSI       float64\n",
            "ATR       float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scaling**"
      ],
      "metadata": {
        "id": "y5QR-aasdmQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "scaled_features = scaler_x.fit_transform(features)\n",
        "scaled_target = scaler_y.fit_transform(target)"
      ],
      "metadata": {
        "id": "EgzHsWFedozK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sequence creation**"
      ],
      "metadata": {
        "id": "RxS3Oej5dvUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(features, target, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(features)):\n",
        "        X.append(features[i-seq_length:i])\n",
        "        y.append(target[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_seq, y_seq = create_sequences(scaled_features, scaled_target)\n",
        "train_size = int(0.8 * len(X_seq))\n",
        "X_train_seq, X_test_seq = X_seq[:train_size], X_seq[train_size:]\n",
        "y_train_seq, y_test_seq = y_seq[:train_size], y_seq[train_size:]\n"
      ],
      "metadata": {
        "id": "xOnX81mCd17U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Definitions**"
      ],
      "metadata": {
        "id": "xpWxQy02d7qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**BiLstm**"
      ],
      "metadata": {
        "id": "w3q7SmaKeA7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bilstm(input_shape):\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=L1L2(0.01, 0.01)), input_shape=input_shape),\n",
        "        Dropout(0.4),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "0KaTDHSheGcG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CNN**"
      ],
      "metadata": {
        "id": "nrR2ndyweK_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
        "        MaxPooling1D(2),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "3TOcYX0UeOqN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Random Forest**"
      ],
      "metadata": {
        "id": "IjhlKl8reTPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rf(X_train, y_train):\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42,verbose=1,          # show progress\n",
        "        n_jobs=-1)\n",
        "    model.fit(X_tree, y_tree.values.ravel())\n",
        "    return model"
      ],
      "metadata": {
        "id": "-OzD8J5OeZWS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**XGBoost**"
      ],
      "metadata": {
        "id": "YUj7eIqJeaav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgboost(X_train, y_train):\n",
        "    model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, verbosity=1, eval_metric='logloss',\n",
        "        use_label_encoder=False)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ezJkRS4UegIB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Training**"
      ],
      "metadata": {
        "id": "Qq6P-x_vepTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for tree models\n",
        "X_tree = features.iloc[30:]\n",
        "y_tree = target.iloc[30:]\n",
        "\n",
        "# Train models\n",
        "print(\"Training BiLSTM...\")\n",
        "bilstm_model = build_bilstm((X_train_seq.shape[1], X_train_seq.shape[2]))\n",
        "bilstm_history = bilstm_model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32,\n",
        "                                  validation_data=(X_test_seq, y_test_seq), verbose=1)\n",
        "\n",
        "print(\"Training CNN...\")\n",
        "cnn_model = build_cnn((X_train_seq.shape[1], X_train_seq.shape[2]))\n",
        "cnn_history = cnn_model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32,\n",
        "                            validation_data=(X_test_seq, y_test_seq), verbose=1)\n",
        "\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = train_rf(X_tree, y_tree.values.ravel())\n",
        "\n",
        "\n",
        "print(\"Training XGBoost...\")\n",
        "xgb_model = train_xgboost(X_tree, y_tree.values.ravel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaQonS9-eoOn",
        "outputId": "41f14013-fad0-40f1-955a-4b2f0aa5cb54"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BiLSTM...\n",
            "Epoch 1/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 139ms/step - loss: 0.3393 - val_loss: 0.0259\n",
            "Epoch 2/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 127ms/step - loss: 0.0121 - val_loss: 0.0259\n",
            "Epoch 3/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 135ms/step - loss: 0.0122 - val_loss: 0.0260\n",
            "Epoch 4/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 132ms/step - loss: 0.0123 - val_loss: 0.0255\n",
            "Epoch 5/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 131ms/step - loss: 0.0123 - val_loss: 0.0258\n",
            "Epoch 6/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 131ms/step - loss: 0.0121 - val_loss: 0.0257\n",
            "Epoch 7/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 130ms/step - loss: 0.0122 - val_loss: 0.0254\n",
            "Epoch 8/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 133ms/step - loss: 0.0123 - val_loss: 0.0256\n",
            "Epoch 9/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 131ms/step - loss: 0.0122 - val_loss: 0.0262\n",
            "Epoch 10/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 131ms/step - loss: 0.0122 - val_loss: 0.0259\n",
            "Epoch 11/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 131ms/step - loss: 0.0123 - val_loss: 0.0256\n",
            "Epoch 12/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 130ms/step - loss: 0.0122 - val_loss: 0.0260\n",
            "Epoch 13/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 131ms/step - loss: 0.0121 - val_loss: 0.0258\n",
            "Epoch 14/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 139ms/step - loss: 0.0123 - val_loss: 0.0253\n",
            "Epoch 15/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 149ms/step - loss: 0.0123 - val_loss: 0.0263\n",
            "Epoch 16/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 138ms/step - loss: 0.0123 - val_loss: 0.0260\n",
            "Epoch 17/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 141ms/step - loss: 0.0123 - val_loss: 0.0264\n",
            "Epoch 18/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 145ms/step - loss: 0.0122 - val_loss: 0.0256\n",
            "Epoch 19/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 135ms/step - loss: 0.0122 - val_loss: 0.0263\n",
            "Epoch 20/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 133ms/step - loss: 0.0123 - val_loss: 0.0262\n",
            "Epoch 21/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 134ms/step - loss: 0.0123 - val_loss: 0.0265\n",
            "Epoch 22/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 133ms/step - loss: 0.0122 - val_loss: 0.0259\n",
            "Epoch 23/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 133ms/step - loss: 0.0123 - val_loss: 0.0262\n",
            "Epoch 24/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 132ms/step - loss: 0.0122 - val_loss: 0.0261\n",
            "Epoch 25/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 134ms/step - loss: 0.0123 - val_loss: 0.0256\n",
            "Epoch 26/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 133ms/step - loss: 0.0122 - val_loss: 0.0262\n",
            "Epoch 27/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 133ms/step - loss: 0.0123 - val_loss: 0.0268\n",
            "Epoch 28/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 131ms/step - loss: 0.0122 - val_loss: 0.0257\n",
            "Epoch 29/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 133ms/step - loss: 0.0123 - val_loss: 0.0255\n",
            "Epoch 30/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 130ms/step - loss: 0.0122 - val_loss: 0.0260\n",
            "Epoch 31/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 141ms/step - loss: 0.0122 - val_loss: 0.0259\n",
            "Epoch 32/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 139ms/step - loss: 0.0123 - val_loss: 0.0258\n",
            "Epoch 33/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 149ms/step - loss: 0.0123 - val_loss: 0.0264\n",
            "Epoch 34/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 136ms/step - loss: 0.0123 - val_loss: 0.0262\n",
            "Epoch 35/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 147ms/step - loss: 0.0122 - val_loss: 0.0261\n",
            "Epoch 36/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 149ms/step - loss: 0.0122 - val_loss: 0.0259\n",
            "Epoch 37/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 139ms/step - loss: 0.0122 - val_loss: 0.0257\n",
            "Epoch 38/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 146ms/step - loss: 0.0121 - val_loss: 0.0259\n",
            "Epoch 39/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 146ms/step - loss: 0.0122 - val_loss: 0.0260\n",
            "Epoch 40/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 145ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 41/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 135ms/step - loss: 0.0122 - val_loss: 0.0261\n",
            "Epoch 42/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 134ms/step - loss: 0.0122 - val_loss: 0.0255\n",
            "Epoch 43/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 134ms/step - loss: 0.0122 - val_loss: 0.0255\n",
            "Epoch 44/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 135ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 45/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 143ms/step - loss: 0.0123 - val_loss: 0.0261\n",
            "Epoch 46/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 136ms/step - loss: 0.0122 - val_loss: 0.0254\n",
            "Epoch 47/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 133ms/step - loss: 0.0122 - val_loss: 0.0263\n",
            "Epoch 48/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 144ms/step - loss: 0.0124 - val_loss: 0.0260\n",
            "Epoch 49/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 154ms/step - loss: 0.0121 - val_loss: 0.0260\n",
            "Epoch 50/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 149ms/step - loss: 0.0123 - val_loss: 0.0261\n",
            "Epoch 51/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 147ms/step - loss: 0.0123 - val_loss: 0.0260\n",
            "Epoch 52/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 135ms/step - loss: 0.0123 - val_loss: 0.0266\n",
            "Epoch 53/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 134ms/step - loss: 0.0123 - val_loss: 0.0266\n",
            "Epoch 54/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 137ms/step - loss: 0.0124 - val_loss: 0.0256\n",
            "Epoch 55/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 144ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 56/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 135ms/step - loss: 0.0121 - val_loss: 0.0259\n",
            "Epoch 57/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 133ms/step - loss: 0.0122 - val_loss: 0.0261\n",
            "Epoch 58/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 135ms/step - loss: 0.0123 - val_loss: 0.0261\n",
            "Epoch 59/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 158ms/step - loss: 0.0122 - val_loss: 0.0264\n",
            "Epoch 60/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 162ms/step - loss: 0.0123 - val_loss: 0.0257\n",
            "Epoch 61/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 161ms/step - loss: 0.0122 - val_loss: 0.0256\n",
            "Epoch 62/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 156ms/step - loss: 0.0123 - val_loss: 0.0267\n",
            "Epoch 63/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 146ms/step - loss: 0.0122 - val_loss: 0.0259\n",
            "Epoch 64/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 155ms/step - loss: 0.0122 - val_loss: 0.0261\n",
            "Epoch 65/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 154ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 66/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 154ms/step - loss: 0.0123 - val_loss: 0.0263\n",
            "Epoch 67/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 153ms/step - loss: 0.0122 - val_loss: 0.0262\n",
            "Epoch 68/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 153ms/step - loss: 0.0121 - val_loss: 0.0256\n",
            "Epoch 69/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 152ms/step - loss: 0.0122 - val_loss: 0.0256\n",
            "Epoch 70/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 143ms/step - loss: 0.0122 - val_loss: 0.0259\n",
            "Epoch 71/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 152ms/step - loss: 0.0123 - val_loss: 0.0266\n",
            "Epoch 72/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 154ms/step - loss: 0.0122 - val_loss: 0.0256\n",
            "Epoch 73/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 151ms/step - loss: 0.0123 - val_loss: 0.0261\n",
            "Epoch 74/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 159ms/step - loss: 0.0122 - val_loss: 0.0258\n",
            "Epoch 75/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 159ms/step - loss: 0.0123 - val_loss: 0.0265\n",
            "Epoch 76/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 161ms/step - loss: 0.0122 - val_loss: 0.0263\n",
            "Epoch 77/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 161ms/step - loss: 0.0121 - val_loss: 0.0255\n",
            "Epoch 78/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 153ms/step - loss: 0.0122 - val_loss: 0.0260\n",
            "Epoch 79/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 161ms/step - loss: 0.0123 - val_loss: 0.0268\n",
            "Epoch 80/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 162ms/step - loss: 0.0122 - val_loss: 0.0261\n",
            "Epoch 81/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 168ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 82/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 165ms/step - loss: 0.0122 - val_loss: 0.0256\n",
            "Epoch 83/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 161ms/step - loss: 0.0122 - val_loss: 0.0260\n",
            "Epoch 84/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 161ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 85/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 160ms/step - loss: 0.0122 - val_loss: 0.0261\n",
            "Epoch 86/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 160ms/step - loss: 0.0123 - val_loss: 0.0256\n",
            "Epoch 87/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 160ms/step - loss: 0.0121 - val_loss: 0.0259\n",
            "Epoch 88/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 152ms/step - loss: 0.0121 - val_loss: 0.0260\n",
            "Epoch 89/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 160ms/step - loss: 0.0122 - val_loss: 0.0262\n",
            "Epoch 90/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 160ms/step - loss: 0.0123 - val_loss: 0.0255\n",
            "Epoch 91/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 160ms/step - loss: 0.0122 - val_loss: 0.0254\n",
            "Epoch 92/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 160ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 93/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 160ms/step - loss: 0.0123 - val_loss: 0.0264\n",
            "Epoch 94/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 161ms/step - loss: 0.0122 - val_loss: 0.0255\n",
            "Epoch 95/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 162ms/step - loss: 0.0123 - val_loss: 0.0260\n",
            "Epoch 96/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 154ms/step - loss: 0.0122 - val_loss: 0.0264\n",
            "Epoch 97/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 162ms/step - loss: 0.0123 - val_loss: 0.0259\n",
            "Epoch 98/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 153ms/step - loss: 0.0123 - val_loss: 0.0260\n",
            "Epoch 99/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 160ms/step - loss: 0.0123 - val_loss: 0.0254\n",
            "Epoch 100/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 161ms/step - loss: 0.0122 - val_loss: 0.0257\n",
            "Training CNN...\n",
            "Epoch 1/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 0.0020 - val_loss: 0.0058\n",
            "Epoch 2/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 9.5908e-04 - val_loss: 0.0045\n",
            "Epoch 3/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 7.5961e-04 - val_loss: 0.0052\n",
            "Epoch 4/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 5.9897e-04 - val_loss: 0.0044\n",
            "Epoch 5/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 4.9678e-04 - val_loss: 0.0051\n",
            "Epoch 6/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 4.7094e-04 - val_loss: 0.0050\n",
            "Epoch 7/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 4.2804e-04 - val_loss: 0.0046\n",
            "Epoch 8/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 3.8993e-04 - val_loss: 0.0047\n",
            "Epoch 9/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 3.6276e-04 - val_loss: 0.0047\n",
            "Epoch 10/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 3.2963e-04 - val_loss: 0.0042\n",
            "Epoch 11/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 3.5387e-04 - val_loss: 0.0047\n",
            "Epoch 12/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 3.1141e-04 - val_loss: 0.0047\n",
            "Epoch 13/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 3.0069e-04 - val_loss: 0.0044\n",
            "Epoch 14/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 2.7853e-04 - val_loss: 0.0046\n",
            "Epoch 15/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.8734e-04 - val_loss: 0.0051\n",
            "Epoch 16/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 2.9386e-04 - val_loss: 0.0057\n",
            "Epoch 17/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 2.7046e-04 - val_loss: 0.0053\n",
            "Epoch 18/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.5342e-04 - val_loss: 0.0047\n",
            "Epoch 19/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.8264e-04 - val_loss: 0.0051\n",
            "Epoch 20/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.5381e-04 - val_loss: 0.0050\n",
            "Epoch 21/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 2.4459e-04 - val_loss: 0.0049\n",
            "Epoch 22/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.4569e-04 - val_loss: 0.0047\n",
            "Epoch 23/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.6648e-04 - val_loss: 0.0044\n",
            "Epoch 24/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.2949e-04 - val_loss: 0.0049\n",
            "Epoch 25/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 2.2645e-04 - val_loss: 0.0044\n",
            "Epoch 26/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.3034e-04 - val_loss: 0.0050\n",
            "Epoch 27/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 2.3012e-04 - val_loss: 0.0049\n",
            "Epoch 28/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.1224e-04 - val_loss: 0.0047\n",
            "Epoch 29/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.2208e-04 - val_loss: 0.0046\n",
            "Epoch 30/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 2.1091e-04 - val_loss: 0.0049\n",
            "Epoch 31/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.0854e-04 - val_loss: 0.0045\n",
            "Epoch 32/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.3159e-04 - val_loss: 0.0051\n",
            "Epoch 33/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.9795e-04 - val_loss: 0.0049\n",
            "Epoch 34/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.2512e-04 - val_loss: 0.0046\n",
            "Epoch 35/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 2.0312e-04 - val_loss: 0.0045\n",
            "Epoch 36/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.9319e-04 - val_loss: 0.0043\n",
            "Epoch 37/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.0157e-04 - val_loss: 0.0046\n",
            "Epoch 38/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.9290e-04 - val_loss: 0.0046\n",
            "Epoch 39/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 2.0208e-04 - val_loss: 0.0044\n",
            "Epoch 40/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.9872e-04 - val_loss: 0.0045\n",
            "Epoch 41/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.8121e-04 - val_loss: 0.0044\n",
            "Epoch 42/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.9269e-04 - val_loss: 0.0045\n",
            "Epoch 43/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.9328e-04 - val_loss: 0.0046\n",
            "Epoch 44/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.9786e-04 - val_loss: 0.0050\n",
            "Epoch 45/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.9065e-04 - val_loss: 0.0044\n",
            "Epoch 46/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.9021e-04 - val_loss: 0.0045\n",
            "Epoch 47/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 2.0534e-04 - val_loss: 0.0046\n",
            "Epoch 48/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.8235e-04 - val_loss: 0.0045\n",
            "Epoch 49/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 2.0188e-04 - val_loss: 0.0050\n",
            "Epoch 50/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.9208e-04 - val_loss: 0.0040\n",
            "Epoch 51/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.9112e-04 - val_loss: 0.0042\n",
            "Epoch 52/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.9284e-04 - val_loss: 0.0043\n",
            "Epoch 53/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.8837e-04 - val_loss: 0.0044\n",
            "Epoch 54/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.8841e-04 - val_loss: 0.0041\n",
            "Epoch 55/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.8815e-04 - val_loss: 0.0044\n",
            "Epoch 56/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.8346e-04 - val_loss: 0.0043\n",
            "Epoch 57/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.7496e-04 - val_loss: 0.0047\n",
            "Epoch 58/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.7983e-04 - val_loss: 0.0046\n",
            "Epoch 59/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.7677e-04 - val_loss: 0.0042\n",
            "Epoch 60/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.7579e-04 - val_loss: 0.0043\n",
            "Epoch 61/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.8056e-04 - val_loss: 0.0042\n",
            "Epoch 62/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1.7531e-04 - val_loss: 0.0044\n",
            "Epoch 63/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.9394e-04 - val_loss: 0.0044\n",
            "Epoch 64/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.7973e-04 - val_loss: 0.0039\n",
            "Epoch 65/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.7765e-04 - val_loss: 0.0042\n",
            "Epoch 66/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.8038e-04 - val_loss: 0.0045\n",
            "Epoch 67/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 1.8894e-04 - val_loss: 0.0043\n",
            "Epoch 68/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.7268e-04 - val_loss: 0.0046\n",
            "Epoch 69/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.7775e-04 - val_loss: 0.0042\n",
            "Epoch 70/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.6927e-04 - val_loss: 0.0042\n",
            "Epoch 71/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.7607e-04 - val_loss: 0.0042\n",
            "Epoch 72/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.7975e-04 - val_loss: 0.0042\n",
            "Epoch 73/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.7538e-04 - val_loss: 0.0044\n",
            "Epoch 74/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.7391e-04 - val_loss: 0.0043\n",
            "Epoch 75/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.9069e-04 - val_loss: 0.0044\n",
            "Epoch 76/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.6243e-04 - val_loss: 0.0045\n",
            "Epoch 77/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.6838e-04 - val_loss: 0.0042\n",
            "Epoch 78/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.9113e-04 - val_loss: 0.0041\n",
            "Epoch 79/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.7532e-04 - val_loss: 0.0041\n",
            "Epoch 80/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6509e-04 - val_loss: 0.0038\n",
            "Epoch 81/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.6745e-04 - val_loss: 0.0045\n",
            "Epoch 82/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.7653e-04 - val_loss: 0.0042\n",
            "Epoch 83/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.7456e-04 - val_loss: 0.0042\n",
            "Epoch 84/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 1.7188e-04 - val_loss: 0.0044\n",
            "Epoch 85/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 1.6008e-04 - val_loss: 0.0042\n",
            "Epoch 86/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 1.8860e-04 - val_loss: 0.0041\n",
            "Epoch 87/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 1.7373e-04 - val_loss: 0.0041\n",
            "Epoch 88/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 1.7409e-04 - val_loss: 0.0039\n",
            "Epoch 89/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 1.6873e-04 - val_loss: 0.0040\n",
            "Epoch 90/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.6643e-04 - val_loss: 0.0040\n",
            "Epoch 91/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.6454e-04 - val_loss: 0.0044\n",
            "Epoch 92/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.7856e-04 - val_loss: 0.0041\n",
            "Epoch 93/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 1.7283e-04 - val_loss: 0.0037\n",
            "Epoch 94/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 1.8467e-04 - val_loss: 0.0061\n",
            "Epoch 95/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.7502e-04 - val_loss: 0.0041\n",
            "Epoch 96/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 1.8249e-04 - val_loss: 0.0039\n",
            "Epoch 97/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 1.7731e-04 - val_loss: 0.0041\n",
            "Epoch 98/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.5845e-04 - val_loss: 0.0040\n",
            "Epoch 99/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6395e-04 - val_loss: 0.0041\n",
            "Epoch 100/100\n",
            "\u001b[1m2210/2210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.7329e-04 - val_loss: 0.0042\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   36.8s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.4min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGBoost...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Evaluation**"
      ],
      "metadata": {
        "id": "pe1mHp8ofGxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation functions\n",
        "def evaluate_model(name, model, X_test, y_test, is_dl=False):\n",
        "    if is_dl:\n",
        "        pred = model.predict(X_test)\n",
        "        if len(pred.shape) > 1: pred = pred.ravel()\n",
        "    else:\n",
        "        pred = model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'MSE': mean_squared_error(y_test, pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, pred)),\n",
        "        'MAE': mean_absolute_error(y_test, pred),\n",
        "        'R2': r2_score(y_test, pred)\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "results = []\n",
        "\n",
        "# DL Models\n",
        "test_dates = data.index[30+train_size:]\n",
        "dl_pred = bilstm_model.predict(X_test_seq)\n",
        "results.append(evaluate_model('BiLSTM', bilstm_model, X_test_seq, y_test_seq, True))\n",
        "results.append(evaluate_model('CNN', cnn_model, X_test_seq, y_test_seq, True))\n",
        "\n",
        "# Tree Models\n",
        "X_test_tree = X_tree.iloc[train_size:]\n",
        "y_test_tree = y_tree.iloc[train_size:]\n",
        "results.append(evaluate_model('Random Forest', rf_model, X_test_tree, y_test_tree))\n",
        "results.append(evaluate_model('XGBoost', xgb_model, X_test_tree, y_test_tree))\n",
        "\n",
        "# Results comparison\n",
        "results_df = pd.DataFrame(results).set_index('Model')\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(results_df.sort_values('RMSE'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXLThWIlfOX0",
        "outputId": "55062b54-6d14-4e3c-85a0-cbf12b699310"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 42ms/step\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Comparison:\n",
            "                        MSE        RMSE        MAE        R2\n",
            "Model                                                       \n",
            "CNN                0.004239    0.065106   0.042166  0.647482\n",
            "BiLSTM             0.016983    0.130319   0.087225 -0.412366\n",
            "Random Forest    904.151494   30.069112  15.636436  0.999992\n",
            "XGBoost        31664.455078  177.945090  88.563812  0.999711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Visualization**"
      ],
      "metadata": {
        "id": "kC-jZR7afqHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(bilstm_history.history['loss'], label='BiLSTM Train')\n",
        "plt.plot(bilstm_history.history['val_loss'], label='BiLSTM Val')\n",
        "plt.plot(cnn_history.history['loss'], label='CNN Train')\n",
        "plt.plot(cnn_history.history['val_loss'], label='CNN Val')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "results_df['RMSE'].sort_values().plot(kind='bar')\n",
        "plt.title('Model RMSE Comparison')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "actual = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1))\n",
        "plt.plot(test_dates, actual, label='Actual')\n",
        "plt.plot(test_dates, scaler_y.inverse_transform(bilstm_model.predict(X_test_seq)), label='BiLSTM')\n",
        "plt.plot(test_dates, scaler_y.inverse_transform(cnn_model.predict(X_test_seq)), label='CNN')\n",
        "plt.title('DL Models Predictions')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(test_dates, actual, label='Actual')\n",
        "plt.plot(test_dates, rf_model.predict(X_test_tree), label='RF')\n",
        "plt.plot(test_dates, xgb_model.predict(X_test_tree), label='XGBoost')\n",
        "plt.title('Tree Models Predictions')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F7pUXQwXfn2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Prediction**"
      ],
      "metadata": {
        "id": "mmyoR8Thf3N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trading signals\n",
        "def generate_signals(predictions, window=5):\n",
        "    signals = []\n",
        "    sma = pd.Series(predictions).rolling(window).mean()\n",
        "    for i in range(len(predictions)):\n",
        "        if i < window:\n",
        "            signals.append('Hold')\n",
        "        elif predictions[i] > sma[i] * 1.02:\n",
        "            signals.append('Buy')\n",
        "        elif predictions[i] < sma[i] * 0.98:\n",
        "            signals.append('Sell')\n",
        "        else:\n",
        "            signals.append('Hold')\n",
        "    return signals\n",
        "\n",
        "# Generate predictions and signals\n",
        "final_preds = {\n",
        "    'Date': test_dates,\n",
        "    'Actual': scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).ravel(),\n",
        "    'BiLSTM': scaler_y.inverse_transform(bilstm_model.predict(X_test_seq).reshape(-1, 1)).ravel(),\n",
        "    'CNN': scaler_y.inverse_transform(cnn_model.predict(X_test_seq).reshape(-1, 1)).ravel(),\n",
        "    'RandomForest': rf_model.predict(X_test_tree),\n",
        "    'XGBoost': xgb_model.predict(X_test_tree)\n",
        "}\n",
        "for model in ['BiLSTM', 'CNN', 'RandomForest', 'XGBoost']:\n",
        "    final_preds[f'{model}_Signal'] = generate_signals(final_preds[model])\n",
        "\n",
        "results_df = pd.DataFrame(final_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_nchYSigEAn",
        "outputId": "0c24a3c5-0980-41b6-e632-0d2b1498ccde"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 42ms/step\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Save results and models**"
      ],
      "metadata": {
        "id": "Y-KsJO26gJcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('stock_predictions_signals.csv', index=False)\n",
        "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
        "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
        "save_model(bilstm_model, 'bilstm_model.h5')\n",
        "save_model(cnn_model, 'cnn_model.h5')\n",
        "\n",
        "print(\"\\nPredictions and signals saved to stock_predictions_signals.csv\")\n",
        "print(\"All models saved to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5rszK_12xQW",
        "outputId": "979db6e2-e556-43a4-d2f5-a643064ccc0e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions and signals saved to stock_predictions_signals.csv\n",
            "All models saved to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Second Comparison**"
      ],
      "metadata": {
        "id": "6EdDEPykg1XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Split Indian and US Data on same model**"
      ],
      "metadata": {
        "id": "ecbmIrzAXLqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Input, Concatenate, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import joblib\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Global configuration\n",
        "plt.style.use('ggplot')\n",
        "MARKET = 'INDIAN'  # Change to 'INDIAN' for Indian market and 'US' for American market\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data(market):\n",
        "    if market == 'INDIAN':\n",
        "        df = pd.read_csv(\"/content/sample_data/Data/merged.csv\")\n",
        "        # df = pd.read_csv(\"/content/sample_data/Data/NIFTY_IT_Historical_Data.csv\")\n",
        "        # Preprocessing steps for Indian market\n",
        "        df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "        df[\"date\"] = pd.to_datetime(df[\"Date\"])\n",
        "        df.set_index(\"date\", inplace=True)\n",
        "        df = df.drop(\"Date\", axis=1)\n",
        "         # Validate and clean columns\n",
        "        required_cols = ['source_file','Open', 'High', 'Low', 'Close']\n",
        "        for col in required_cols:\n",
        "            df[col] = (df[col].astype(str)\n",
        "                      .str.replace(r'[^\\d.-]', '', regex=True)\n",
        "                      .replace(r'^\\.$', np.nan, regex=True)\n",
        "                      .replace('', np.nan))\n",
        "            # df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Add synthetic volume for demonstration\n",
        "        df['Volume'] = np.random.randint(10000, 50000, size=len(df))\n",
        "        df.dropna(inplace=True)\n",
        "        return df\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    data = load_data(MARKET)\n",
        "    # data = add_technical_indicators(data)"
      ],
      "metadata": {
        "id": "p1bvnbVHQnPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load merged data\n",
        "merged_df = df\n",
        "\n",
        "# Add a fallback mapping of features to market sectors\n",
        "sector_mapping = {\n",
        "    'Open': 'Price',\n",
        "    'High': 'Price',\n",
        "    'Low': 'Price',\n",
        "    'Close': 'Price',\n",
        "    'Volume': 'Volume',\n",
        "    'SMA_10': 'Technical',\n",
        "    'EMA_10': 'Technical',\n",
        "    'RSI': 'Technical',\n",
        "    'ATR': 'Technical',\n",
        "    'Twitter_Sentiment': 'Social Media',\n",
        "    'Reddit_Sentiment': 'Social Media',\n",
        "    'Social_Volume': 'Social Media',\n",
        "    # Add others as needed\n",
        "}\n",
        "\n",
        "# Identify feature columns (exclude target and identifiers)\n",
        "feature_cols = merged_df.columns.difference(['source_file', 'Date', 'source_file_Date'])\n",
        "\n",
        "# Group data by source_file\n",
        "grouped = merged_df.groupby('source_file')"
      ],
      "metadata": {
        "id": "-pqvtY1tRCj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['source_file'].unique()"
      ],
      "metadata": {
        "id": "z6VjXHszaJ5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize SHAP explainer for tree models\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "\n",
        "# Use a sample from test set to keep visualization manageable\n",
        "X_sample = rf_features_test[:200]  # limit size for speed\n",
        "\n",
        "# Calculate SHAP values\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_sample, feature_names=feature_names,max_display=10)\n",
        "\n",
        "\n",
        "# shap.summary_plot(shap_values, X_sample, feature_names=feature_names)\n"
      ],
      "metadata": {
        "id": "P8NIrpJ65Qao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Concatenate, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import os\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Global configuration\n",
        "plt.style.use('ggplot')\n",
        "MARKET = 'US'  # Set to 'US' or 'INDIAN'\n",
        "US_STOCK_FILE = \"/content/sample_data/Data/stock_yfinance_data.csv\"\n",
        "US_SOCIAL_FILE = \"/content/sample_data/Data/stock_tweets.csv\"\n",
        "INDIAN_FILE = \"/content/sample_data/Data/NIFTY_IT_Historical_Data.csv\"\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data(market):\n",
        "    if market == 'INDIAN':\n",
        "        # Load Indian market data\n",
        "        df = pd.read_csv(INDIAN_FILE)\n",
        "\n",
        "        # Preprocessing steps for Indian market\n",
        "        df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "        df[\"date\"] = pd.to_datetime(df[\"Date\"])\n",
        "        df.set_index(\"date\", inplace=True)\n",
        "        df = df.drop(\"Date\", axis=1, errors='ignore')\n",
        "\n",
        "        # Validate and clean columns\n",
        "        required_cols = ['Open', 'High', 'Low', 'Close']\n",
        "        for col in required_cols:\n",
        "            df[col] = (df[col].astype(str)\n",
        "                      .str.replace(r'[^\\d.-]', '', regex=True)\n",
        "                      .replace(r'^\\.$', np.nan, regex=True)\n",
        "                      .replace('', np.nan))\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Add synthetic volume if missing\n",
        "        if 'Volume' not in df.columns:\n",
        "            df['Volume'] = np.random.randint(10000, 50000, size=len(df))\n",
        "        df.dropna(inplace=True)\n",
        "        return df\n",
        "\n",
        "    elif market == 'US':\n",
        "        # Load US stock data\n",
        "        #stock_df = pd.read_csv(US_STOCK_FILE)\n",
        "        # Handle mixed date formats\n",
        "        # 1. Verify file exists\n",
        "        if not os.path.exists(US_STOCK_FILE):\n",
        "          raise FileNotFoundError(f\"Data file not found at: {US_STOCK_FILE}\")\n",
        "\n",
        "        # 2. Read CSV with error handling\n",
        "        try:\n",
        "           stock_df = pd.read_csv(US_STOCK_FILE)\n",
        "        except Exception as e:\n",
        "            raise IOError(f\"Error reading {US_STOCK_FILE}: {str(e)}\")\n",
        "\n",
        "        # 3. Check if DataFrame is empty\n",
        "        if stock_df.empty:\n",
        "            raise ValueError(f\"Loaded empty DataFrame from {US_STOCK_FILE}\")\n",
        "\n",
        "        # 4. Handle date parsing with better validation\n",
        "        if 'Date' not in stock_df.columns:\n",
        "            raise KeyError(\"CSV file is missing 'Date' column\")\n",
        "\n",
        "        stock_df['Date'] = pd.to_datetime(\n",
        "            stock_df['Date'],\n",
        "            errors='coerce',\n",
        "            format='mixed',\n",
        "            dayfirst=True\n",
        "        )\n",
        "        # 5. Handle failed date parsing\n",
        "        date_failures = stock_df['Date'].isna().sum()\n",
        "        if date_failures > 0:\n",
        "            print(f\"Warning: Failed to parse {date_failures} date values\")\n",
        "            stock_df = stock_df.dropna(subset=['Date'])\n",
        "\n",
        "        # 6. Final validation before return\n",
        "        if stock_df.empty:\n",
        "            raise ValueError(\"All rows were dropped during date parsing\")\n",
        "\n",
        "        # 7. Sort by date for time-series operations\n",
        "        stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
        "        # Remove rows with unparseable dates\n",
        "        initial_count = len(stock_df)\n",
        "        stock_df = stock_df.dropna(subset=['Date'])\n",
        "        final_count = len(stock_df)\n",
        "\n",
        "        if initial_count != final_count:\n",
        "          print(f\"Warning: Dropped {initial_count - final_count} rows with unparseable dates\")\n",
        "        stock_df['Date'] = pd.to_datetime(stock_df['Date'], format='%Y-%m-%d')\n",
        "        stock_df.columns = stock_df.columns.str.strip().str.replace(' ', '_')\n",
        "        stock_df[\"date\"] = pd.to_datetime(stock_df[\"Date\"], format='%d-%m-%Y')\n",
        "        stock_df.set_index(\"date\", inplace=True)\n",
        "        stock_df = stock_df.drop([\"Date\", \"Adj_Close\", \"Stock_Name\"], axis=1, errors='ignore')\n",
        "\n",
        "        # Load social media data\n",
        "        social_df = pd.read_csv(US_SOCIAL_FILE)\n",
        "        social_df['Date'] = social_df['Date'].str.split(pat = \" \").str[0]\n",
        "        social_df['Date'] = pd.to_datetime(social_df['Date'], format='%Y-%m-%d')\n",
        "        social_df.columns = social_df.columns.str.strip().str.replace(' ', '_')\n",
        "        social_df[\"date\"] = pd.to_datetime(social_df[\"Date\"])\n",
        "        social_df.set_index(\"date\", inplace=True)\n",
        "        social_df = social_df.drop(\"Date\", axis=1, errors='ignore')\n",
        "\n",
        "        # Merge stock and social data\n",
        "        df = pd.merge(stock_df, social_df, left_index=True, right_index=True, how='left')\n",
        "\n",
        "        # Fill missing social data with zeros\n",
        "        social_cols = ['Twitter_Sentiment', 'Reddit_Sentiment', 'Social_Volume']\n",
        "        for col in social_cols:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0.0\n",
        "            else:\n",
        "                df[col].fillna(0, inplace=True)\n",
        "\n",
        "        # Clean numerical columns\n",
        "        for col in ['Open', 'High', 'Low', 'Close', 'Volume'] + social_cols:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        df.dropna(inplace=True)\n",
        "        return df\n",
        "\n",
        "# Technical indicators\n",
        "def add_technical_indicators(df):\n",
        "    if df is None or df.empty:\n",
        "        raise ValueError(\"Received invalid DataFrame in add_technical_indicators\")\n",
        "    # Moving Averages\n",
        "    df['SMA_10'] = df['Close'].rolling(10).mean()\n",
        "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    # RSI\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    avg_gain = gain.rolling(14).mean()\n",
        "    avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # ATR\n",
        "    high_low = df['High'] - df['Low']\n",
        "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "    df['ATR'] = true_range.rolling(14).mean()\n",
        "\n",
        "    return df.dropna()\n",
        "\n",
        "# FINESS Hybrid Model Architecture\n",
        "def create_hybrid_model(tech_input_shape, social_input_shape=None):\n",
        "    # Technical Feature Processor (Tech+)\n",
        "    tech_input = Input(shape=tech_input_shape, name='tech_input')\n",
        "    x = LSTM(128, return_sequences=True)(tech_input)\n",
        "    x = LSTM(64, return_sequences=False)(x)\n",
        "    tech_output = Dense(32, activation='relu')(x)\n",
        "\n",
        "    # Social Feature Processor (FINESS) - Only for US market\n",
        "    if social_input_shape is not None:\n",
        "        social_input = Input(shape=social_input_shape, name='social_input')\n",
        "        y = LSTM(64, return_sequences=False)(social_input)\n",
        "        social_output = Dense(16, activation='relu')(y)\n",
        "\n",
        "        # Combine technical and social features\n",
        "        combined = Concatenate()([tech_output, social_output])\n",
        "        z = Dense(64, activation='relu')(combined)\n",
        "    else:\n",
        "        z = tech_output\n",
        "\n",
        "    # Final prediction layers\n",
        "    z = Dense(32, activation='relu')(z)\n",
        "    z = Dropout(0.3)(z)\n",
        "    output = Dense(1)(z)\n",
        "\n",
        "    # Create model\n",
        "    if social_input_shape is not None:\n",
        "        model = Model(inputs=[tech_input, social_input], outputs=output)\n",
        "    else:\n",
        "        model = Model(inputs=tech_input, outputs=output)\n",
        "\n",
        "    model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Create sequences for LSTM\n",
        "def create_sequences(data, target_col, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(data)):\n",
        "        X.append(data.iloc[i-seq_length:i].values)\n",
        "        y.append(data.iloc[i][target_col])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Inverse scaling function\n",
        "def inverse_scale(scaler, values, feature_index):\n",
        "    dummy = np.zeros((len(values), scaler.n_features_in_))\n",
        "    dummy[:, feature_index] = values\n",
        "    return scaler.inverse_transform(dummy)[:, feature_index]\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    data = load_data(MARKET)\n",
        "    data = add_technical_indicators(data)\n",
        "    print(f\"Data shape after preprocessing: {data.shape}\")\n",
        "    print(f\"Columns: {data.columns.tolist()}\")\n",
        "\n",
        "    # Prepare features based on market\n",
        "    if MARKET == 'US':\n",
        "        tech_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'EMA_10', 'RSI', 'ATR']\n",
        "        social_features = ['Twitter_Sentiment', 'Reddit_Sentiment', 'Social_Volume']\n",
        "        target = 'Close'\n",
        "    else:  # Indian market\n",
        "        tech_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'EMA_10', 'RSI', 'ATR']\n",
        "        social_features = None\n",
        "        target = 'Close'\n",
        "\n",
        "    # Train-test split (time-based)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_data = data.iloc[:train_size]\n",
        "    test_data = data.iloc[train_size:]\n",
        "\n",
        "    # Scale data\n",
        "    tech_scaler = MinMaxScaler()\n",
        "    train_tech = tech_scaler.fit_transform(train_data[tech_features])\n",
        "    test_tech = tech_scaler.transform(test_data[tech_features])\n",
        "\n",
        "    if MARKET == 'US':\n",
        "        social_scaler = MinMaxScaler()\n",
        "        train_social = social_scaler.fit_transform(train_data[social_features])\n",
        "        test_social = social_scaler.transform(test_data[social_features])\n",
        "\n",
        "    # Create sequences\n",
        "    seq_length = 30\n",
        "    close_idx = tech_features.index('Close')  # Index of Close for inverse scaling\n",
        "\n",
        "    X_train_tech, y_train = create_sequences(\n",
        "        pd.DataFrame(train_tech, columns=tech_features),\n",
        "        target_col=close_idx,\n",
        "        seq_length=seq_length\n",
        "    )\n",
        "\n",
        "    X_test_tech, y_test = create_sequences(\n",
        "        pd.DataFrame(test_tech, columns=tech_features),\n",
        "        target_col=close_idx,\n",
        "        seq_length=seq_length\n",
        "    )\n",
        "\n",
        "    if MARKET == 'US':\n",
        "        X_train_social, _ = create_sequences(\n",
        "            pd.DataFrame(train_social, columns=social_features),\n",
        "            target_col=0,\n",
        "            seq_length=seq_length\n",
        "        )\n",
        "        X_test_social, _ = create_sequences(\n",
        "            pd.DataFrame(test_social, columns=social_features),\n",
        "            target_col=0,\n",
        "            seq_length=seq_length\n",
        "        )\n",
        "\n",
        "    # Build and train hybrid model\n",
        "    if MARKET == 'US':\n",
        "        model = create_hybrid_model(\n",
        "            tech_input_shape=(seq_length, len(tech_features)),\n",
        "            social_input_shape=(seq_length, len(social_features))\n",
        "        )\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=15, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(factor=0.1, patience=5)\n",
        "        ]\n",
        "        print(\"\\nTraining hybrid model with social features...\")\n",
        "        history = model.fit(\n",
        "            [X_train_tech, X_train_social], y_train,\n",
        "            validation_data=([X_test_tech, X_test_social], y_test),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "    else:\n",
        "        model = create_hybrid_model(\n",
        "            tech_input_shape=(seq_length, len(tech_features))\n",
        "        )\n",
        "        print(\"\\nTraining hybrid model without social features...\")\n",
        "        history = model.fit(\n",
        "            X_train_tech, y_train,\n",
        "            validation_data=(X_test_tech, y_test),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    # Train Random Forest for comparison\n",
        "    print(\"\\nTraining Random Forest model...\")\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    # Prepare 2D features for RF\n",
        "    rf_features_train = X_train_tech.reshape(X_train_tech.shape[0], -1)\n",
        "    rf_features_test = X_test_tech.reshape(X_test_tech.shape[0], -1)\n",
        "\n",
        "    rf.fit(rf_features_train, y_train)\n",
        "    rf_pred = rf.predict(rf_features_test)\n",
        "\n",
        "    # Generate predictions\n",
        "    if MARKET == 'US':\n",
        "        hybrid_pred = model.predict([X_test_tech, X_test_social]).flatten()\n",
        "    else:\n",
        "        hybrid_pred = model.predict(X_test_tech).flatten()\n",
        "\n",
        "    # Inverse scale predictions to original price values\n",
        "    y_test_actual = inverse_scale(tech_scaler, y_test, close_idx)\n",
        "    hybrid_pred_actual = inverse_scale(tech_scaler, hybrid_pred, close_idx)\n",
        "    rf_pred_actual = inverse_scale(tech_scaler, rf_pred, close_idx)\n",
        "\n",
        "    # Evaluate models\n",
        "    def evaluate_model(name, actual, predicted):\n",
        "        return {\n",
        "            'Model': name,\n",
        "            'MSE': mean_squared_error(actual, predicted),\n",
        "            'RMSE': np.sqrt(mean_squared_error(actual, predicted)),\n",
        "            'MAE': mean_absolute_error(actual, predicted),\n",
        "            'R2': r2_score(actual, predicted)\n",
        "        }\n",
        "\n",
        "    results = [\n",
        "        evaluate_model('Hybrid Model', y_test_actual, hybrid_pred_actual),\n",
        "        evaluate_model('Random Forest', y_test_actual, rf_pred_actual)\n",
        "    ]\n",
        "\n",
        "    results_df = pd.DataFrame(results).set_index('Model')\n",
        "    print(\"\\nModel Evaluation Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot training progress\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Training Progress')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot predictions vs actual\n",
        "    plt.subplot(2, 2, 2)\n",
        "    test_dates = test_data.index[seq_length:seq_length+len(y_test_actual)]\n",
        "    plt.plot(test_dates, y_test_actual, label='Actual', linewidth=2)\n",
        "    plt.plot(test_dates, hybrid_pred_actual, label='Hybrid Prediction', linestyle='--')\n",
        "    plt.plot(test_dates, rf_pred_actual, label='RF Prediction', linestyle='--')\n",
        "    plt.title(f'{MARKET} Market: Actual vs Predicted Prices')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Plot feature importance for Random Forest\n",
        "    plt.subplot(2, 2, 3)\n",
        "    # Create names for features with time steps\n",
        "    feature_names = []\n",
        "    for i in range(seq_length-1, -1, -1):\n",
        "        for feat in tech_features:\n",
        "            feature_names.append(f\"t-{i}_{feat}\")\n",
        "\n",
        "    importances = rf.feature_importances_\n",
        "    n_top = min(20, len(importances))\n",
        "    indices = np.argsort(importances)[-n_top:]\n",
        "\n",
        "    plt.title(f'Top {n_top} Important Features (Random Forest)')\n",
        "    plt.barh(range(n_top), importances[indices], align='center')\n",
        "    plt.yticks(range(n_top), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Relative Importance')\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.scatter(y_test_actual, hybrid_pred_actual, alpha=0.6, label='Hybrid Model')\n",
        "    plt.scatter(y_test_actual, rf_pred_actual, alpha=0.6, label='Random Forest')\n",
        "    plt.plot([min(y_test_actual), max(y_test_actual)],\n",
        "             [min(y_test_actual), max(y_test_actual)],\n",
        "             'k--', lw=2)\n",
        "    plt.xlabel('Actual Prices')\n",
        "    plt.ylabel('Predicted Prices')\n",
        "    plt.title('Actual vs Predicted Prices')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'finess_results_{MARKET}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Save models\n",
        "    model.save(f'finess_hybrid_model_{MARKET}.h5')\n",
        "    joblib.dump(rf, f'random_forest_model_{MARKET}.pkl')\n",
        "    joblib.dump(tech_scaler, f'tech_scaler_{MARKET}.pkl')\n",
        "    if MARKET == 'US':\n",
        "        joblib.dump(social_scaler, f'social_scaler_{MARKET}.pkl')\n",
        "    print(\"\\nModels and scalers saved successfully!\")"
      ],
      "metadata": {
        "id": "KuWFX1XKJAP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import necessary packages after kernel reset\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load cleaned data again\n",
        "file_path = \"/content/sample_data/Data/merged.csv\"\n",
        "df_real = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'Open', 'High', 'Low' to numeric\n",
        "for col in ['Open', 'High', 'Low']:\n",
        "    df_real[col] = pd.to_numeric(df_real[col], errors='coerce')\n",
        "\n",
        "# Drop rows with any missing values in critical columns\n",
        "df_real_clean = df_real.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "# Define target\n",
        "median_close = df_real_clean['Close'].median()\n",
        "df_real_clean['target'] = (df_real_clean['Close'] > median_close).astype(int)\n",
        "\n",
        "# Downsample to 10,000 rows\n",
        "df_sampled = df_real_clean.sample(n=10000, random_state=42)\n",
        "\n",
        "# Features and target\n",
        "X_sampled = df_sampled[['Open', 'High', 'Low', 'Volume', 'source_file']].copy()\n",
        "y_sampled = df_sampled['target']\n",
        "\n",
        "# Encode 'source_file' as numeric\n",
        "X_sampled['source_file'] = pd.factorize(X_sampled['source_file'])[0]\n",
        "\n",
        "# Train/test split\n",
        "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = train_test_split(\n",
        "    X_sampled, y_sampled, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train RandomForest model\n",
        "model_sampled = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
        "model_sampled.fit(X_train_sampled, y_train_sampled)\n",
        "\n",
        "# SHAP TreeExplainer\n",
        "explainer_sampled = shap.TreeExplainer(model_sampled)\n",
        "shap_values_sampled = explainer_sampled.shap_values(X_test_sampled)\n",
        "\n",
        "# Plot SHAP summary\n",
        "shap.summary_plot(shap_values_sampled, X_test_sampled, show=False)\n",
        "plt.tight_layout()\n",
        "output_path_sampled = \"sampled_shap_plot.png\"\n",
        "plt.savefig(output_path_sampled)\n",
        "plt.close()\n",
        "\n",
        "output_path_sampled\n"
      ],
      "metadata": {
        "id": "oAGvel-e9ow7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Input, Concatenate, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import joblib\n",
        "import os\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Global configuration\n",
        "plt.style.use('ggplot')\n",
        "MARKET = 'INDIAN'  # Change to 'INDIAN' for Indian market and 'US' for American market\n",
        "US_STOCK_FILE = \"/content/sample_data/Data/stock_yfinance_data.csv\"\n",
        "US_SOCIAL_FILE = \"/content/sample_data/Data/stock_tweets.csv\"\n",
        "INDIAN_FILE = \"/content/sample_data/Data/merged.csv\"\n",
        "\n",
        "# Fixed load_data function for Indian market\n",
        "def load_data(market):\n",
        "    if market == 'INDIAN':\n",
        "        try:\n",
        "            print(f\"Loading Indian market data from: {INDIAN_FILE}\")\n",
        "            df = pd.read_csv(INDIAN_FILE)\n",
        "            print(f\"Initial data shape: {df.shape}\")\n",
        "\n",
        "            # 1. Clean and process columns\n",
        "            price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "            for col in price_cols:\n",
        "                if col in df.columns:\n",
        "                    # Clean the column\n",
        "                    df[col] = df[col].astype(str)\n",
        "                    df[col] = df[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
        "                    df[col] = df[col].replace(r'^\\.$', np.nan, regex=True)\n",
        "                    df[col] = df[col].replace('', np.nan)\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            # 2. Create Source_File column if missing\n",
        "            if 'Source_File' not in df.columns:\n",
        "                # Use filename as source file\n",
        "                df['Source_File'] = os.path.basename(INDIAN_FILE)\n",
        "\n",
        "            # 3. Sector mapping logic\n",
        "            print(\"Mapping sectors...\")\n",
        "            sector_mapping = {\n",
        "                'NIFTY NEXT 50': 'Large Cap',\n",
        "                'NIFTY 50': 'Large Cap',\n",
        "                'NIFTY PHARMA': 'Pharmaceuticals',\n",
        "                'NIFTY FMCG': 'FMCG',\n",
        "                'NIFTY 100': 'Large Cap',\n",
        "                'NIFTY IT': 'Information Technology',\n",
        "                'NIFTY HOUSING': 'Housing',\n",
        "                'NIFTY INDIA MANUFACTURING': 'Manufacturing',\n",
        "                'NIFTY BANK': 'Banking',\n",
        "                'NIFTY MIDCAP 100': 'Mid Cap',\n",
        "                'NIFTY INFRASTRUCTURE': 'Infrastructure',\n",
        "                'NIFTY PSU BANK': 'Public Sector Banking',\n",
        "                'NIFTY ENERGY': 'Energy',\n",
        "                'NIFTY AUTO': 'Automobile',\n",
        "                'NIFTY MEDIA': 'Media',\n",
        "                'NIFTY METAL': 'Metals',\n",
        "                'NIFTY COMMODITIES': 'Commodities',\n",
        "                'NIFTY PRIVATE BANK': 'Private Banking',\n",
        "                'NIFTY OIL  GAS': 'Oil & Gas',\n",
        "                'VIX': 'Volatility',\n",
        "                'DEFAULT': 'Other'\n",
        "            }\n",
        "\n",
        "            def map_sector(source_file):\n",
        "                source_file = str(source_file).upper()\n",
        "                for key, sector in sector_mapping.items():\n",
        "                    if key in source_file:\n",
        "                        return sector\n",
        "                return sector_mapping['DEFAULT']\n",
        "\n",
        "            df['Sector'] = df['Source_File'].apply(map_sector)\n",
        "            print(f\"Sectors mapped: {df['Sector'].nunique()} unique sectors\")\n",
        "            print(\"Sample sectors:\", df['Sector'].unique()[:5])\n",
        "\n",
        "            # 4. Convert and sort dates\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df.sort_values('Date', inplace=True)\n",
        "            df.set_index('Date', inplace=True)\n",
        "\n",
        "            print(f\"Final data shape: {df.shape}\")\n",
        "            print(\"Sample columns:\", df.columns.tolist()[:5])\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    elif market == 'US':\n",
        "        # (US market code remains the same as in your original)\n",
        "        # ... [unchanged US market code] ...\n",
        "        return df  # Make sure to return df for US market\n",
        "\n",
        "# Fixed technical indicators function\n",
        "def add_technical_indicators(df):\n",
        "    # Skip calculation if no numerical columns exist\n",
        "    if len(df.select_dtypes(include=[np.number]).columns) == 0:\n",
        "        print(\"Warning: No numerical columns for technical indicators\")\n",
        "        return df\n",
        "    # Calculate indicators for each sector\n",
        "    sectors = df['Sector'].unique() if 'Sector' in df.columns else ['DEFAULT']\n",
        "\n",
        "    for sector in sectors:\n",
        "        # Skip if sector columns don't exist\n",
        "        if f'{sector}_Close' not in df.columns:\n",
        "            continue\n",
        "\n",
        "        # Simple Moving Average\n",
        "        df[f'{sector}_SMA_10'] = df[f'{sector}_Close'].rolling(10).mean()\n",
        "\n",
        "        # Exponential Moving Average\n",
        "        df[f'{sector}_EMA_10'] = df[f'{sector}_Close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "        # RSI\n",
        "        delta = df[f'{sector}_Close'].diff()\n",
        "        gain = delta.clip(lower=0)\n",
        "        loss = -delta.clip(upper=0)\n",
        "        avg_gain = gain.rolling(14).mean()\n",
        "        avg_loss = loss.rolling(14).mean()\n",
        "        rs = avg_gain / (avg_loss + 1e-10)\n",
        "        df[f'{sector}_RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # ATR\n",
        "        high = df[f'{sector}_High'] if f'{sector}_High' in df.columns else df[f'{sector}_Close']\n",
        "        low = df[f'{sector}_Low'] if f'{sector}_Low' in df.columns else df[f'{sector}_Close']\n",
        "        close = df[f'{sector}_Close']\n",
        "        tr1 = high - low\n",
        "        tr2 = (high - close.shift()).abs()\n",
        "        tr3 = (low - close.shift()).abs()\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        df[f'{sector}_ATR'] = tr.rolling(14).mean()\n",
        "\n",
        "    # Fill any remaining NaNs\n",
        "    df = df.ffill().bfill().dropna()\n",
        "    return df\n",
        "\n",
        "# FINESS Hybrid Model Architecture\n",
        "def create_hybrid_model(tech_input_shape, social_input_shape=None):\n",
        "    # Technical Feature Processor (Tech+)\n",
        "    tech_input = Input(shape=tech_input_shape, name='tech_input')\n",
        "    x = LSTM(128, return_sequences=True)(tech_input)\n",
        "    x = LSTM(64, return_sequences=False)(x)\n",
        "    tech_output = Dense(32, activation='relu')(x)\n",
        "\n",
        "    # Social Feature Processor (FINESS) - Only for US market\n",
        "    if social_input_shape and MARKET == 'US':\n",
        "        social_input = Input(shape=social_input_shape, name='social_input')\n",
        "        y = LSTM(64, return_sequences=False)(social_input)\n",
        "        social_output = Dense(16, activation='relu')(y)\n",
        "\n",
        "        # Combine technical and social features\n",
        "        combined = Concatenate()([tech_output, social_output])\n",
        "        z = Dense(64, activation='relu')(combined)\n",
        "    else:\n",
        "        z = tech_output\n",
        "\n",
        "    # Final prediction layers\n",
        "    z = Dense(32, activation='relu')(z)\n",
        "    z = Dropout(0.3)(z)\n",
        "    output = Dense(1)(z)\n",
        "\n",
        "    # Create model\n",
        "    if MARKET == 'US':\n",
        "        model = Model(inputs=[tech_input, social_input], outputs=output)\n",
        "    else:\n",
        "        model = Model(inputs=tech_input, outputs=output)\n",
        "\n",
        "    model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Create sequences for LSTM (fixed to handle different input types)\n",
        "def create_sequences(data, target_col, seq_length=30):\n",
        "    X, y = [], []\n",
        "\n",
        "    # Handle both DataFrame and array inputs\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        values = data.values\n",
        "    else:\n",
        "        values = data\n",
        "    if MARKET == 'INDIAN':\n",
        "        try:\n",
        "            # Try to find target column index in features\n",
        "            target_idx = tech_features.index(target_col)\n",
        "        except ValueError:\n",
        "            # If not found, use last column as fallback\n",
        "            target_idx = -1\n",
        "            print(f\"Warning: Target column {target_col} not in features, using last column\")\n",
        "\n",
        "    # Check if target_col is index or column name\n",
        "    if isinstance(target_col, str) and isinstance(data, pd.DataFrame):\n",
        "        target_idx = data.columns.get_loc(target_col)\n",
        "    else:\n",
        "        target_idx = target_col\n",
        "\n",
        "    for i in range(seq_length, len(values)):\n",
        "        X.append(values[i-seq_length:i])\n",
        "        y.append(values[i][target_idx])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    data = load_data(MARKET)\n",
        "\n",
        "    # Diagnostic output\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATA LOADING DIAGNOSTICS:\")\n",
        "    print(f\"Market: {MARKET}\")\n",
        "    print(f\"Data type: {type(data)}\")\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Columns: {data.columns.tolist()[:5]}...\")\n",
        "        print(f\"Index type: {type(data.index)}\")\n",
        "        if not data.empty:\n",
        "            print(f\"Date range: {data.index.min()} to {data.index.max()}\")\n",
        "        else:\n",
        "            print(\"DataFrame is empty\")\n",
        "    else:\n",
        "        print(f\"Data is not a DataFrame: {type(data)}\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Validate before proceeding\n",
        "    if data is None or data.empty:\n",
        "        raise ValueError(\"Data loading failed. Check file path and format.\")\n",
        "\n",
        "    # Add technical indicators\n",
        "    data = add_technical_indicators(data)\n",
        "\n",
        "    # Prepare features based on market\n",
        "    if MARKET == 'US':\n",
        "        tech_features = ['Open', 'High', 'Low', 'Volume', 'SMA_10', 'EMA_10', 'RSI', 'ATR']\n",
        "        social_features = ['Twitter_Sentiment', 'Reddit_Sentiment', 'Social_Volume']\n",
        "        target = 'Close'\n",
        "    else:\n",
        "        # Use only numerical columns and exclude categorical columns\n",
        "        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        # Define target - try to find a Close column\n",
        "        close_cols = [col for col in numerical_cols if 'Close' in col]\n",
        "        target_col = close_cols[0] if close_cols else 'Close'\n",
        "\n",
        "        # If no Close column exists, create one\n",
        "        if target_col not in data.columns:\n",
        "            data['Close'] = data['Open']  # Fallback to Open price\n",
        "            numerical_cols.append('Close')\n",
        "            target_col = 'Close'\n",
        "\n",
        "        tech_features = numerical_cols\n",
        "\n",
        "        # Ensure target column is not in features\n",
        "        if target_col in tech_features:\n",
        "            tech_features.remove(target_col)\n",
        "\n",
        "        print(f\"Using target column: {target_col}\")\n",
        "        print(f\"Technical features: {tech_features}\")\n",
        "\n",
        "    # Train-test split (time-based)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_data = data.iloc[:train_size]\n",
        "    test_data = data.iloc[train_size:]\n",
        "\n",
        "    # Scale data\n",
        "    tech_scaler = MinMaxScaler()\n",
        "    train_tech = tech_scaler.fit_transform(train_data[tech_features])\n",
        "    test_tech = tech_scaler.transform(test_data[tech_features])\n",
        "\n",
        "    if MARKET == 'US':\n",
        "        social_scaler = StandardScaler()\n",
        "        train_social = social_scaler.fit_transform(train_data[social_features])\n",
        "        test_social = social_scaler.transform(test_data[social_features])\n",
        "\n",
        "    # Create sequences\n",
        "    seq_length = 30\n",
        "\n",
        "    # For Indian market, use column index of target\n",
        "    if MARKET == 'INDIAN':\n",
        "        target_idx = tech_features.index(target_col) if target_col in tech_features else 0\n",
        "    else:\n",
        "        target_idx = tech_features.index(target) if target in tech_features else 0\n",
        "\n",
        "    X_train_tech, y_train = create_sequences(\n",
        "        pd.DataFrame(train_tech, columns=tech_features),\n",
        "        target_col=target_idx,\n",
        "        seq_length=seq_length\n",
        "    )\n",
        "\n",
        "    X_test_tech, y_test = create_sequences(\n",
        "        pd.DataFrame(test_tech, columns=tech_features),\n",
        "        target_col=target_idx,\n",
        "        seq_length=seq_length\n",
        "    )\n",
        "\n",
        "    if MARKET == 'US':\n",
        "        X_train_social, _ = create_sequences(\n",
        "            pd.DataFrame(train_social, columns=social_features),\n",
        "            target_col=0,  # First column as placeholder\n",
        "            seq_length=seq_length\n",
        "        )\n",
        "        X_test_social, _ = create_sequences(\n",
        "            pd.DataFrame(test_social, columns=social_features),\n",
        "            target_col=0,\n",
        "            seq_length=seq_length\n",
        "        )\n",
        "\n",
        "    # Build and train hybrid model\n",
        "    if MARKET == 'US':\n",
        "        model = create_hybrid_model(\n",
        "            tech_input_shape=(seq_length, len(tech_features)),\n",
        "            social_input_shape=(seq_length, len(social_features))\n",
        "        )\n",
        "        history = model.fit(\n",
        "            [X_train_tech, X_train_social], y_train,\n",
        "            validation_data=([X_test_tech, X_test_social], y_test),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            callbacks=[]#[EarlyStopping(patience=10)]\n",
        "        )\n",
        "    else:\n",
        "        model = create_hybrid_model(\n",
        "            tech_input_shape=(seq_length, len(tech_features))\n",
        "        )\n",
        "        history = model.fit(\n",
        "            X_train_tech, y_train,\n",
        "            validation_data=(X_test_tech, y_test),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            callbacks=[]#[EarlyStopping(patience=10)]\n",
        "        )\n",
        "\n",
        "    # Train Random Forest for comparison\n",
        "    rf = RandomForestRegressor(n_estimators=50, random_state=42)  # Reduced for faster testing\n",
        "\n",
        "    # Prepare 2D features for RF\n",
        "    rf_features_train = X_train_tech.reshape(X_train_tech.shape[0], -1)\n",
        "    rf_features_test = X_test_tech.reshape(X_test_tech.shape[0], -1)\n",
        "\n",
        "    rf.fit(rf_features_train, y_train)\n",
        "    rf_pred = rf.predict(rf_features_test)\n",
        "\n",
        "    # Generate predictions\n",
        "    if MARKET == 'US':\n",
        "        hybrid_pred = model.predict([X_test_tech, X_test_social]).flatten()\n",
        "    else:\n",
        "        hybrid_pred = model.predict(X_test_tech).flatten()\n",
        "\n",
        "    # Evaluate models\n",
        "    def evaluate_model(name, actual, predicted):\n",
        "        return {\n",
        "            'Model': name,\n",
        "            'MSE': mean_squared_error(actual, predicted),\n",
        "            'RMSE': np.sqrt(mean_squared_error(actual, predicted)),\n",
        "            'MAE': mean_absolute_error(actual, predicted),\n",
        "            'R2': r2_score(actual, predicted)\n",
        "        }\n",
        "\n",
        "    results = [\n",
        "        evaluate_model('Hybrid Model', y_test, hybrid_pred),\n",
        "        evaluate_model('Random Forest', y_test, rf_pred)\n",
        "    ]\n",
        "\n",
        "    results_df = pd.DataFrame(results).set_index('Model')\n",
        "    print(\"\\nModel Evaluation Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Plot training progress\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Training Progress')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot predictions vs actual\n",
        "    plt.subplot(1, 2, 2)\n",
        "    test_dates = test_data.index[seq_length:seq_length+len(y_test)]\n",
        "    plt.plot(test_dates, y_test, label='Actual', alpha=0.7)\n",
        "    plt.plot(test_dates, hybrid_pred, label='Hybrid Prediction', alpha=0.7)\n",
        "    plt.plot(test_dates, rf_pred, label='RF Prediction', alpha=0.7)\n",
        "    plt.title(f'{MARKET} Market Predictions')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # SHAP Analysis (simplified)\n",
        "    try:\n",
        "        # Prepare SHAP explainer\n",
        "        explainer = shap.TreeExplainer(rf)\n",
        "        shap_values = explainer.shap_values(rf_features_test)\n",
        "\n",
        "        # Plot SHAP summary\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        shap.summary_plot(shap_values, rf_features_test, feature_names=tech_features)\n",
        "        plt.title(f'SHAP Feature Importance for {MARKET} Market')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'shap_summary_{MARKET}.png', dpi=300)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP analysis failed: {str(e)}\")\n",
        "\n",
        "    # Feature importance for Random Forest\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    importances = rf.feature_importances_\n",
        "\n",
        "    # Get top features\n",
        "    n_top = min(15, len(importances))\n",
        "    indices = np.argsort(importances)[-n_top:]\n",
        "\n",
        "    # Create meaningful feature names\n",
        "    feature_names = []\n",
        "    for i in range(seq_length):\n",
        "        for j, feature in enumerate(tech_features):\n",
        "            feature_names.append(f\"t-{seq_length-i-1}_{feature}\")\n",
        "\n",
        "    plt.title(f'Top {n_top} Important Features (Random Forest)')\n",
        "    plt.barh(range(n_top), importances[indices], align='center')\n",
        "    plt.yticks(range(n_top), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Relative Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{MARKET}_market_feature_importance.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Save models\n",
        "    model.save(f'finess_hybrid_model_{MARKET}.h5')\n",
        "    joblib.dump(rf, f'random_forest_model_{MARKET}.pkl')\n",
        "    print(\"Models saved successfully!\")"
      ],
      "metadata": {
        "id": "yvotYdQYL5bO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}